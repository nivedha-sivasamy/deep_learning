{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass Classification using Tensorflow\n",
        "\n",
        "## Aim: To implement a perceptron to classify the given data using tensorflow.\n",
        "\n",
        "## Dataset: Iris dataset."
      ],
      "metadata": {
        "id": "Zgm859fTTrRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_FzC46zB-qt",
        "outputId": "20104a86-9590-4dd8-8d48-5ca2f1c5fd87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LFkurU8IKie",
        "outputId": "e6830f52-e9e5-4405-aa96-4fba117fa6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200 Cost:  0.34226292\n",
            "Epoch 400 Cost:  0.22627068\n",
            "Epoch 600 Cost:  0.17976946\n",
            "Epoch 800 Cost:  0.15393177\n",
            "Epoch 1000 Cost:  0.13714094\n",
            "Epoch 1200 Cost:  0.12544718\n",
            "Epoch 1400 Cost:  0.11684302\n",
            "Epoch 1600 Cost:  0.110263444\n",
            "Epoch 1800 Cost:  0.10506229\n",
            "Epoch 2000 Cost:  0.100889176\n",
            "Optimization Finished\n",
            "Accuracy:  100.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.disable_v2_behavior()\n",
        "tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "\n",
        "def label_encoder(label):\n",
        "  value =[]\n",
        "  if label ==\"Iris-setosa\":\n",
        "    value = [1,0,0]\n",
        "  elif label ==\"Iris-versicolor\":\n",
        "    value = [0,1,0]\n",
        "  elif label ==\"Iris-virginica\":\n",
        "    value =[0,0,1]\n",
        "  return value\n",
        "\n",
        "\n",
        "def data_encode(file):\n",
        "  X = []\n",
        "  Y = []\n",
        "  train_file =open(file, \"r\")\n",
        "  for line in train_file.read().strip().split(\"\\n\"):\n",
        "    line = line.split(\",\")\n",
        "    X.append([line[0], line[1],line[2],line[3]])\n",
        "    Y.append(label_encoder(line[4]))\n",
        "  return X, Y\n",
        "\n",
        "def model(input, weights, bias):\n",
        "  layer_1 = tf.add(tf.matmul(input, weights[\"hidden\"]), bias[\"hidden\"])\n",
        "  layer_1 =tf.nn.relu(layer_1)\n",
        "  output_layer =tf.matmul(layer_1, weights[\"output\"])+bias[\"output\"]\n",
        "  return output_layer\n",
        "\n",
        "X_train, Y_train = data_encode(\"/content/drive/MyDrive/Colab Notebooks/3rd sem DL/iris.train\")\n",
        "X_test, Y_test = data_encode(\"/content/drive/MyDrive/Colab Notebooks/3rd sem DL/iris.test\")\n",
        "learning_rate =0.01\n",
        "training_epochs = 2000\n",
        "display_steps = 200\n",
        "n_input= 4\n",
        "n_hidden = 10\n",
        "n_output = 3\n",
        "\n",
        "X = tf.compat.v1.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.compat.v1.placeholder(\"float\", [None, n_output])\n",
        "\n",
        "weights = {\n",
        "    \"hidden\": tf.Variable(tf.random.normal([n_input, n_hidden]), name =\"Weights_hidden\"),\n",
        "    \"output\": tf.Variable(tf.random.normal([n_hidden,n_output]), name = \"wieghts_output\")\n",
        "}\n",
        "\n",
        "bias = {\n",
        "    \"hidden\":tf.Variable(tf.random.normal([n_hidden]), name=\"bias_hidden\"),\n",
        "    \"output\": tf.Variable(tf.random.normal([n_output]), name=\"bias_output\")\n",
        "}\n",
        "\n",
        "pred = model(X, weights, bias)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits =pred, labels =Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.compat.v1.Session() as sess:\n",
        "  sess.run(init)\n",
        "  for epoch in range(training_epochs):\n",
        "    i,c = sess.run([optimizer, cost], feed_dict={X:X_train, Y:Y_train})\n",
        "    if(epoch+1)% display_steps ==0:\n",
        "      print(\"Epoch\", epoch+1, \"Cost: \", c)\n",
        "\n",
        "  print(\"Optimization Finished\")\n",
        "\n",
        "  test_result =sess.run(pred, feed_dict={X:X_test})\n",
        "  correct_pred =tf.equal(tf.argmax(test_result,1), tf.argmax(Y_test, 1))\n",
        "  accuracy =tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
        "  ac= accuracy.eval({X:X_test, Y:Y_test})\n",
        "  print(\"Accuracy: \", round(ac*100,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion: We can observe as that the loss or the cost function decreases as the epochs increases and we obtain an accuracy of 100%."
      ],
      "metadata": {
        "id": "Mp-TaDXHPEzz"
      }
    }
  ]
}